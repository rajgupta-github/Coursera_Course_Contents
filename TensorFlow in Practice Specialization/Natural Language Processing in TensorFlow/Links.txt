
https://rishabhmisra.github.io/publications/

https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js

Please find the link to he IMDB reviews dataset here
http://ai.stanford.edu/~amaas/data/sentiment/
You will find here 50,000 movie reviews which are classified as positive of negative.

http://projector.tensorflow.org/

https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder

This week you looked at taking your tokenized words and using Embeddings to establish meaning from them in a mathematical way. 
Words were mapped to vectors in higher dimensional space, and the semantics of the words then learned when those words were labelled with 
similar meaning. So, for example, when looking at movie reviews, those movies with positive sentiment had the dimensionality of their words 
ending up ‘pointing’ a particular way, and those with negative sentiment pointing in a different direction. 
From this, the words in future sentences could have their ‘direction’ established, and from this the sentiment inferred. 
You then looked at sub word tokenization, and saw that not only do the meanings of the words matter, but also the sequence in which 
they are found.


https://www.kaggle.com/kazanova/sentiment140
https://nlp.stanford.edu/projects/glove/

https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt

https://www.tensorflow.org/tutorials/text/text_generation


